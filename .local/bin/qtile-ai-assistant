#!/usr/bin/env python3
"""
Qtile AI Assistant - Comprehensive desktop AI interface
Integrates voice processing, LLM interactions, and system control
"""

import sys
import os
import json
import asyncio
import subprocess
import tempfile
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Union
import logging

# Configure logging
log_dir = Path.home() / ".local/share/mcp-logs"
log_dir.mkdir(parents=True, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / "qtile-ai.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class QtileAI:
    def __init__(self):
        self.home = Path.home()
        self.temp_dir = Path("/tmp/qtile-ai")
        self.temp_dir.mkdir(exist_ok=True)

        # Audio settings
        self.audio_file = self.temp_dir / "voice_input.wav"
        self.text_file = self.temp_dir / "voice_text.txt"

        # MCP settings
        self.mcp_servers_dir = self.home / ".local/share/mcp-servers"

        # System commands
        self.commands = {
            "volume_up": "amixer set Master 10%+",
            "volume_down": "amixer set Master 10%-",
            "volume_mute": "amixer set Master toggle",
            "brightness_up": "brightnessctl s +10%",
            "brightness_down": "brightnessctl s 10%-",
            "screenshot": "scrot -s ~/Pictures/Screenshots/%Y-%m-%d_%H-%M-%S.png",
            "lock_screen": "i3lock -c 000000",
            "suspend": "systemctl suspend"
        }

        # AI prompt templates
        self.prompts = {
            "system": """You are a helpful desktop AI assistant integrated with qtile window manager.
You can control media playback, system settings, and assist with various tasks.
Be concise and actionable in your responses.""",
            "voice_context": "Process this voice command and provide a brief response: ",
            "chat_context": "Desktop assistance request: "
        }

    def notify(self, title: str, message: str, urgency: str = "normal"):
        """Send desktop notification"""
        try:
            cmd = ["dunstify", "-t", "3000", "-u", urgency, title, message]
            subprocess.run(cmd, capture_output=True, text=True, timeout=10)
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError) as e:
            logger.error(f"Notification failed: {e}")
            # Fallback to console
            print(f"{title}: {message}")

    def speak(self, text: str):
        """Text-to-speech output"""
        try:
            # Clean text for speech
            clean_text = text.replace('\n', ' ').strip()
            if clean_text:
                subprocess.run(
                    ["espeak-ng", "-s", "150", "-v", "en", clean_text],
                    capture_output=True, timeout=10
                )
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError) as e:
            logger.error(f"Speech failed: {e}")

    def record_audio(self, duration: int = 4) -> bool:
        """Record audio for voice input"""
        try:
            # Clear notification and prompt to start speaking
            self.notify("AI Assistant - Voice Mode", f"üó£Ô∏è Start speaking now! ({duration}s)", "normal")
            self.speak("Start speaking now")

            # Small delay to let TTS finish
            import time
            time.sleep(1)

            # Record using arecord
            cmd = ["timeout", str(duration), "arecord", "-f", "cd", "-t", "wav", str(self.audio_file)]
            result = subprocess.run(cmd, capture_output=True, text=True)

            if self.audio_file.exists() and self.audio_file.stat().st_size > 1000:
                self.notify("AI Assistant", "‚úÖ Recording complete, processing...", "normal")
                return True
            else:
                self.notify("AI Assistant", "‚ùå No audio recorded or too quiet", "critical")
                return False

        except Exception as e:
            logger.error(f"Audio recording failed: {e}")
            self.notify("AI Assistant", f"Recording error: {str(e)}", "critical")
            return False

    def transcribe_audio(self, audio_file: Path) -> str:
        """Transcribe audio to text (placeholder for now)"""
        # For now, we'll prompt the user for input
        # In a real implementation, this would use a speech-to-text service
        try:
            # Try using whisper if available
            if subprocess.run(["which", "whisper"], capture_output=True).returncode == 0:
                result = subprocess.run(
                    ["whisper", str(audio_file), "--model", "tiny", "--output_format", "txt"],
                    capture_output=True, text=True, timeout=30
                )
                if result.returncode == 0:
                    txt_file = audio_file.with_suffix('.txt')
                    if txt_file.exists():
                        return txt_file.read_text().strip()

            # Fallback to manual input
            self.notify("AI Assistant", "Enter your command:")
            import tkinter as tk
            from tkinter import simpledialog

            root = tk.Tk()
            root.withdraw()
            command = simpledialog.askstring("AI Assistant", "What would you like me to do?")
            root.destroy()

            return command or ""

        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            return ""

    def execute_system_command(self, command_key: str) -> bool:
        """Execute predefined system commands"""
        if command_key not in self.commands:
            return False

        try:
            cmd = self.commands[command_key].split()
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except Exception as e:
            logger.error(f"System command failed: {e}")
            return False

    def call_mcp_tool(self, server: str, tool: str, args: Dict = None) -> str:
        """Call MCP server tool"""
        try:
            if server == "mpris":
                server_path = self.mcp_servers_dir / "mpris-server.py"
                if not server_path.exists():
                    return "MPRIS server not found"

                # For now, use playerctl directly
                if tool == "play":
                    result = subprocess.run(["playerctl", "play"], capture_output=True, text=True)
                elif tool == "pause":
                    result = subprocess.run(["playerctl", "pause"], capture_output=True, text=True)
                elif tool == "play_pause":
                    result = subprocess.run(["playerctl", "play-pause"], capture_output=True, text=True)
                elif tool == "next":
                    result = subprocess.run(["playerctl", "next"], capture_output=True, text=True)
                elif tool == "previous":
                    result = subprocess.run(["playerctl", "previous"], capture_output=True, text=True)
                elif tool == "get_status":
                    status_result = subprocess.run(["playerctl", "status"], capture_output=True, text=True)
                    title_result = subprocess.run(["playerctl", "metadata", "title"], capture_output=True, text=True)
                    artist_result = subprocess.run(["playerctl", "metadata", "artist"], capture_output=True, text=True)

                    return f"Status: {status_result.stdout.strip()}\nTitle: {title_result.stdout.strip()}\nArtist: {artist_result.stdout.strip()}"
                else:
                    return f"Unknown MPRIS tool: {tool}"

                return result.stdout.strip() if result.returncode == 0 else f"Error: {result.stderr.strip()}"

        except Exception as e:
            logger.error(f"MCP call failed: {e}")
            return f"Error calling {server}.{tool}: {str(e)}"

    def process_natural_language_command(self, text: str) -> str:
        """Process natural language commands and route to appropriate tools"""
        text_lower = text.lower().strip()

        # Media control commands
        if any(word in text_lower for word in ["play", "resume", "start"]):
            if "next" in text_lower:
                result = self.call_mcp_tool("mpris", "next")
                return f"Skipped to next track. {result}"
            elif "previous" in text_lower or "prev" in text_lower:
                result = self.call_mcp_tool("mpris", "previous")
                return f"Went to previous track. {result}"
            else:
                result = self.call_mcp_tool("mpris", "play")
                return f"Playing media. {result}"

        elif any(word in text_lower for word in ["pause", "stop"]):
            result = self.call_mcp_tool("mpris", "pause")
            return f"Paused media. {result}"

        elif "status" in text_lower or "what" in text_lower and "playing" in text_lower:
            return self.call_mcp_tool("mpris", "get_status")

        # Volume control
        elif "volume" in text_lower:
            if "up" in text_lower or "increase" in text_lower or "louder" in text_lower:
                if self.execute_system_command("volume_up"):
                    return "Volume increased"
                else:
                    return "Failed to increase volume"
            elif "down" in text_lower or "decrease" in text_lower or "quiet" in text_lower:
                if self.execute_system_command("volume_down"):
                    return "Volume decreased"
                else:
                    return "Failed to decrease volume"
            elif "mute" in text_lower or "silent" in text_lower:
                if self.execute_system_command("volume_mute"):
                    return "Volume toggled"
                else:
                    return "Failed to mute volume"

        # Brightness control
        elif "brightness" in text_lower or "screen" in text_lower:
            if "up" in text_lower or "increase" in text_lower or "brighter" in text_lower:
                if self.execute_system_command("brightness_up"):
                    return "Brightness increased"
                else:
                    return "Failed to increase brightness"
            elif "down" in text_lower or "decrease" in text_lower or "dimmer" in text_lower:
                if self.execute_system_command("brightness_down"):
                    return "Brightness decreased"
                else:
                    return "Failed to decrease brightness"

        # Screenshot
        elif "screenshot" in text_lower or "capture" in text_lower or "snap" in text_lower:
            if self.execute_system_command("screenshot"):
                return "Screenshot taken"
            else:
                return "Failed to take screenshot"

        # System control
        elif "lock" in text_lower:
            if self.execute_system_command("lock_screen"):
                return "Screen locked"
            else:
                return "Failed to lock screen"

        elif "suspend" in text_lower or "sleep" in text_lower:
            if self.execute_system_command("suspend"):
                return "System suspended"
            else:
                return "Failed to suspend system"

        # Default: pass to Emacs MCP
        else:
            try:
                # Send to Emacs for LLM processing
                emacs_cmd = f"emacsclient --eval '(+mcp/process-voice-command \"{text}\")'"
                result = subprocess.run(emacs_cmd, shell=True, capture_output=True, text=True, timeout=30)
                if result.returncode == 0:
                    return result.stdout.strip().strip('"') or "Command processed by AI assistant"
                else:
                    return "AI processing temporarily unavailable"
            except Exception as e:
                logger.error(f"Emacs MCP processing failed: {e}")
                return f"I didn't understand: '{text}'. Try commands like 'play music', 'volume up', 'take screenshot'"

    def voice_mode(self):
        """Handle voice input workflow"""
        try:
            # Initial activation notification
            self.notify("AI Assistant - Voice Mode", "üé§ Voice assistant activated")

            # Record audio (this will show the "start speaking" notification)
            if not self.record_audio(duration=4):
                return

            # Transcribe audio
            transcribed_text = self.transcribe_audio(self.audio_file)
            if not transcribed_text:
                self.notify("AI Assistant", "‚ùå Could not understand audio", "critical")
                self.speak("I didn't understand that")
                return

            logger.info(f"Voice command: {transcribed_text}")

            # Process command
            response = self.process_natural_language_command(transcribed_text)

            # Respond
            self.notify("AI Assistant", f"‚úÖ {response}")
            self.speak(response)

            logger.info(f"Response: {response}")

        except Exception as e:
            logger.error(f"Voice mode error: {e}")
            self.notify("AI Assistant", f"Voice processing error: {str(e)}", "critical")
            self.speak("Sorry, there was an error")

        finally:
            # Cleanup
            if self.audio_file.exists():
                self.audio_file.unlink()

    def chat_mode(self):
        """Handle text chat input workflow via Emacs gptel"""
        try:
            self.notify("AI Assistant - Chat Mode", "üó®Ô∏è Opening full-screen AI chat...")

            # Launch Emacs with gptel in full-frame mode
            # Use the +gptel/here function from doom emacs
            emacs_cmd = [
                "emacsclient",
                "-c",
                "-a", "emacs",
                "--eval", "(progn (delete-other-windows) (+gptel/here) (rename-buffer \"*AI Desktop Assistant*\"))"
            ]

            # Run emacs in background so script can continue
            subprocess.Popen(emacs_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            logger.info("Launched Emacs gptel chat interface")

        except Exception as e:
            logger.error(f"Chat mode error: {e}")
            self.notify("AI Assistant", f"Failed to open chat: {str(e)}", "critical")

            # Fallback: try basic emacs frame
            try:
                fallback_cmd = ["emacsclient", "-c", "-a", "emacs"]
                subprocess.Popen(fallback_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                self.notify("AI Assistant", "Opened Emacs (gptel may not be available)", "normal")
            except Exception as fallback_error:
                logger.error(f"Fallback chat failed: {fallback_error}")
                self.notify("AI Assistant", "Unable to open chat interface", "critical")

    def status_mode(self):
        """Show AI assistant status"""
        try:
            # Check MCP servers
            mpris_status = "‚úÖ Running" if (self.mcp_servers_dir / "mpris-server.py").exists() else "‚ùå Not found"

            # Check system tools
            tools_status = []
            for tool in ["playerctl", "amixer", "espeak-ng", "dunstify"]:
                status = "‚úÖ" if subprocess.run(["which", tool], capture_output=True).returncode == 0 else "‚ùå"
                tools_status.append(f"{tool}: {status}")

            status_text = f"""AI Assistant Status:

MPRIS Server: {mpris_status}

System Tools:
{chr(10).join(tools_status)}

Logs: {log_dir / 'qtile-ai.log'}
"""

            self.notify("AI Assistant Status", status_text)
            print(status_text)

        except Exception as e:
            logger.error(f"Status check failed: {e}")

def main():
    parser = argparse.ArgumentParser(description="Qtile AI Assistant")
    parser.add_argument("mode", choices=["voice", "chat", "status"], help="Operation mode")
    args = parser.parse_args()

    ai = QtileAI()

    if args.mode == "voice":
        ai.voice_mode()
    elif args.mode == "chat":
        ai.chat_mode()
    elif args.mode == "status":
        ai.status_mode()

if __name__ == "__main__":
    main()